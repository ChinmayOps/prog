{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f877ab3-86f1-46c2-8f91-7c19d2725845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Lambda\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15d3b5ca-2967-4289-92e3-9e1940610b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Corpus: [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog'], ['i', 'love', 'deep', 'learning', 'and', 'neural', 'networks'], ['natural', 'language', 'processing', 'is', 'fascinating']]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Data Preparation\n",
    "corpus = [\n",
    "    \"The quick brown fox jumped over the lazy dog\",\n",
    "    \"I love deep learning and neural networks\",\n",
    "    \"Natural language processing is fascinating\"\n",
    "]\n",
    "\n",
    "# Convert to lowercase\n",
    "corpus = [sentence.lower().split() for sentence in corpus]\n",
    "print(\"Tokenized Corpus:\", corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "079ea27e-49e9-4063-aad8-94094b0152fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary: {'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumped': 5, 'over': 6, 'lazy': 7, 'dog': 8, 'i': 9, 'love': 10, 'deep': 11, 'learning': 12, 'and': 13, 'neural': 14, 'networks': 15, 'natural': 16, 'language': 17, 'processing': 18, 'is': 19, 'fascinating': 20}\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Build Vocabulary\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([' '.join(sentence) for sentence in corpus])\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "vocab_size = len(word2id) + 1\n",
    "print(\"\\nVocabulary:\", word2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b17ef5da-630c-4e75-bf12-291ad9a4eaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample (context -> target):\n",
      "Context: ['quick', 'brown'] → Target: the\n",
      "Context: ['quick', 'brown'] → Target: the\n",
      "Context: ['the', 'brown', 'fox'] → Target: quick\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Generate Training Data (CBOW)\n",
    "window_size = 2\n",
    "data = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    sentence_ids = [word2id[word] for word in sentence]\n",
    "    for idx, target in enumerate(sentence_ids):\n",
    "        start = max(0, idx - window_size)\n",
    "        end = min(len(sentence_ids), idx + window_size + 1)\n",
    "        context = [sentence_ids[i] for i in range(start, end) if i != idx]\n",
    "        for word in context:\n",
    "            data.append((context, target))\n",
    "\n",
    "print(\"\\nSample (context -> target):\")\n",
    "for i in range(3):\n",
    "    ctx_words = [id2word[w] for w in data[i][0]]\n",
    "    tgt_word = id2word[data[i][1]]\n",
    "    print(f\"Context: {ctx_words} → Target: {tgt_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a2c1bf-1895-44a6-aef9-a2de24cba488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Prepare Input and Output Arrays\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for context, target in data:\n",
    "    X.append(context)\n",
    "    y.append(target)\n",
    "\n",
    "# Pad / reshape context\n",
    "max_context_len = 2 * window_size\n",
    "X = np.array([np.pad(x, (0, max_context_len - len(x)), constant_values=0) for x in X])\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09ef74e7-0746-473f-ab8a-b18139e5de7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">210</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">231</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m10\u001b[0m)          │           \u001b[38;5;34m210\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m)             │           \u001b[38;5;34m231\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">441</span> (1.72 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m441\u001b[0m (1.72 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">441</span> (1.72 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m441\u001b[0m (1.72 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 6: Define CBOW Model\n",
    "embedding_dim = 10\n",
    "input_layer = Input(shape=(max_context_len,))\n",
    "embedding = Embedding(vocab_size, embedding_dim, input_length=max_context_len)(input_layer)\n",
    "mean = Lambda(lambda x: K.mean(x, axis=1))(embedding)\n",
    "output = Dense(vocab_size, activation='softmax')(mean)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d440c8d-a153-4722-b5e3-e64a11266ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model training completed!\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Train the model\n",
    "history = model.fit(X, y, epochs=50, verbose=0)\n",
    "print(\"\\nModel training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "201de64a-405a-46cf-96ad-f63ab939499c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Embedding for 'deep':\n",
      "[-0.09790551 -0.14152099  0.02024945 -0.06452678  0.09479229 -0.0211451\n",
      "  0.07026905  0.08406613 -0.03262172 -0.01925941]\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Output word embeddings\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "print(\"\\nWord Embedding for 'deep':\")\n",
    "print(weights[word2id['deep']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ddf4e5-3f22-4cfb-a928-f7847a37fb92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
